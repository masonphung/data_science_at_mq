{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9pWsPAt0dCh"
   },
   "source": [
    "# Macquarie University\n",
    "\n",
    "- COMP6420 - AI for Text and Vision \n",
    "- Assignment 3, Part 2 - Find complex answers to medical questions\n",
    "- 47828013 - Mason Phung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "import keras_tuner as kt\n",
    "import keras_nlp\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from keras import ops\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Flatten, Dense, Dropout, Lambda, Layer, Embedding, MultiHeadAttention, LayerNormalization, GlobalAveragePooling1D\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.preprocessing import sequence\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives of this assignment\n",
    "\n",
    "In assignment 3 you will work on a general answer selection task. Given a question and a list of sentences, the final goal is to predict which of these sentences from the list can be used as part of the answer to the question. Assignment 3 is divided into two parts. Part 1 will help you get familiar with the data, and Part 2 requires you to implement deep neural networks.\n",
    "\n",
    "The data is in the file `train.csv`, which is provided in both GitHub repository and in iLearn. Each row of the file consists of a question ('qtext' column), an answer ('atext' column), and a label ('label' column) that indicates whether the  answer is correctly related to the question (1) or not (0).\n",
    "\n",
    "The following code uses pandas to store the file `train.csv` in a data frame and shows the first few rows of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtext</th>\n",
       "      <th>label</th>\n",
       "      <th>atext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What are the symptoms of gastritis?</td>\n",
       "      <td>1</td>\n",
       "      <td>However, the most common symptoms include: Nau...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What are the symptoms of gastritis?</td>\n",
       "      <td>0</td>\n",
       "      <td>var s_context; s_context= s_context || {}; s_c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of gastritis?</td>\n",
       "      <td>0</td>\n",
       "      <td>!s_sensitive, chron ID: $('article embeded_mod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What does the treatment for gastritis involve?</td>\n",
       "      <td>1</td>\n",
       "      <td>Treatment for gastritis usually involves: Taki...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What does the treatment for gastritis involve?</td>\n",
       "      <td>1</td>\n",
       "      <td>Eliminating irritating foods from your diet su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            qtext  label  \\\n",
       "0             What are the symptoms of gastritis?      1   \n",
       "1             What are the symptoms of gastritis?      0   \n",
       "2             What are the symptoms of gastritis?      0   \n",
       "3  What does the treatment for gastritis involve?      1   \n",
       "4  What does the treatment for gastritis involve?      1   \n",
       "\n",
       "                                               atext  \n",
       "0  However, the most common symptoms include: Nau...  \n",
       "1  var s_context; s_context= s_context || {}; s_c...  \n",
       "2  !s_sensitive, chron ID: $('article embeded_mod...  \n",
       "3  Treatment for gastritis usually involves: Taki...  \n",
       "4  Eliminating irritating foods from your diet su...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import data, changed orginal 'dataset' name to 'train_data'\n",
    "# I unzip the data then read each file following the example, which directly read the train data, not the zip.\n",
    "test_data = pd.read_csv(\"test.csv\")\n",
    "val_data = pd.read_csv(\"val.csv\")\n",
    "train_data = pd.read_csv(\"train.csv\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qtext</th>\n",
       "      <th>atext</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5039</td>\n",
       "      <td>5039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4341</td>\n",
       "      <td>4341</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       qtext  atext\n",
       "label              \n",
       "0       5039   5039\n",
       "1       4341   4341"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check data balance\n",
    "train_data.groupby('label').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the left-most index is not part of the data, it is added by ipynb automatically for easy reading. You can also browse the data using Microsoft Excel or similar software."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now let's get started.\n",
    "\n",
    "Use the provided files `train.csv`, `val.csv`, and `test.csv` in the data.zip file for all the tasks below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VTTgRnN0dC4"
   },
   "source": [
    "# Task 1 (8 marks): Simple Siamese NN - Contrastive Loss\n",
    "\n",
    "✅ = check as done\n",
    "\n",
    "Implement a simple TensorFlow-Keras neural model that meets the following requirements:\n",
    "\n",
    "1. ✅(0.5 marks) An input layer that will accept the tf.idf of paired data. The input of the Siamese network is a pair of data, i.e., (qtext, atext).\n",
    "2. ✅(2 marks) Use two hidden layers and a ReLU activation function. You need to determine the size of the hidden layers in {64, 128, 256} using val data, assuming these two layers use the same hidden size.\n",
    "3. ✅(0.5 marks) Use Euclidean-distance-based contrastive loss to train the model.\n",
    "4. ✅(0.5 marks) Use Sigmoid function for classification.\n",
    "5. ✅(1 mark) Calculate prediction accuracy.\n",
    "6. ✅(1.5 marks) Give an example of failure case, and explain the possible reason and discuss potential solution. \n",
    "7. (1 mark) Good coding style as explained in the above Assessment Section.\n",
    "8. (1 mark) Correctly feeding data into your model, and correctly training and testing of your models.\n",
    "\n",
    "Use the test data to report the final accuracy of your best model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's create utility functions**\n",
    "\n",
    "We use these to calculate essential metrics for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(vector):\n",
    "    \"\"\"\n",
    "    Calculate the euclidean distance between two vectors.\n",
    "    \n",
    "    Parameters\n",
    "    - vector: A list containing two input tensors (vectors) to compute the distance between.\n",
    "    \n",
    "    Returns:\n",
    "        A tensor representing the Euclidean distance between the two input vectors.\n",
    "    \"\"\"\n",
    "\t# compute the sum of squared distances between the vectors\n",
    "    sumSquared = K.sum(K.square(vector[0] - vector[1]), axis=1, keepdims=True)\n",
    "\t# return the euclidean distance between the vectors\n",
    "    return K.sqrt(K.maximum(sumSquared, K.epsilon()))\n",
    "\n",
    "def contrastive_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Utilize the function from the practical class w10.\n",
    "    I made a small change when calling margin.\n",
    "    Calculates the contrastive loss.\n",
    "\n",
    "    Parameters:\n",
    "    - y_true: List of labels, each label is of type float32.\n",
    "    - y_pred: List of predictions of same length as of y_true,\n",
    "                each label is of type float32.\n",
    "\n",
    "    Returns:\n",
    "        A tensor containing contrastive loss as floating point value.\n",
    "    \"\"\"\n",
    "    # Directly set margin=1\n",
    "    margin = 1\n",
    "    square_pred = K.square(y_pred)\n",
    "    margin_square = K.square(K.maximum(margin - (y_pred), 0))\n",
    "    return K.mean((1 - y_true) * 0.5*square_pred + (y_true)*0.5 * margin_square)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing\n",
    "\n",
    "Process the data by vectorizing the text data, fit & transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uY6sDbUn0dC6"
   },
   "outputs": [],
   "source": [
    "# Use TF-IDF to vectorize the text data\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# The text data is split into two columns: qtext and atext\n",
    "# Do fit_transform on the training data and transform on the validation and test data\n",
    "qtrain_tfidf = vectorizer.fit_transform(train_data['qtext']).toarray()\n",
    "atrain_tfidf = vectorizer.transform(train_data['atext']).toarray()\n",
    "qval_tfidf = vectorizer.transform(val_data['qtext']).toarray()\n",
    "aval_tfidf = vectorizer.transform(val_data['atext']).toarray()\n",
    "qtest_tfidf = vectorizer.transform(test_data['qtext']).toarray()\n",
    "atest_tfidf = vectorizer.transform(test_data['atext']).toarray()\n",
    "\n",
    "# Convert labels to arrays\n",
    "train_label = np.array(train_data['label'])\n",
    "val_label = np.array(val_data['label'])\n",
    "test_label = np.array(test_data['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the base model\n",
    "\n",
    "Seperate this building part with other parts in case we need to build a different base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_model(hidden_size):\n",
    "    \"\"\"\n",
    "    Build a base neural network model for processing input vectors.\n",
    "    \n",
    "    Parameters:\n",
    "    - hidden_size: An integer specifying the number of units in the hidden layers.\n",
    "    \n",
    "    Returns:\n",
    "        A Keras model consisting of two hidden layers with a ReLU activation.\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    # Two hidden layers and a relu activation function\n",
    "    model.add(Dense(units = hidden_size, activation='relu', name = \"relu_layer\"))\n",
    "    model.add(Dense(units = hidden_size, activation=None, name = \"dense_layer\"))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a siamese model including:\n",
    "- The base model\n",
    "- Contrastive loss\n",
    "- Hidden layer size tuning {64, 128, 256}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siam_model(hp):\n",
    "    \"\"\"\n",
    "    Construct a Siamese neural network model.\n",
    "    Hidden size is ready to be hp tuned.\n",
    "    \n",
    "    Parameters:\n",
    "    - hp(HyperParameters object): For tuning model configurations.\n",
    "    \n",
    "    Returns:\n",
    "        A compiled Keras Model for Siamese architecture that takes two input vectors and outputs a similarity score.\n",
    "    \"\"\"\n",
    "    # Build the base model with hidden size hp tuning\n",
    "    hidden_size = hp.Choice(name = \"hidden_size\", values = [64,128,256])\n",
    "    network = base_model(hidden_size)\n",
    "    \n",
    "    # Two input layers of two data\n",
    "    input_qtext = Input(shape=(qtrain_tfidf.shape[1],))\n",
    "    input_atext = Input(shape=(atrain_tfidf.shape[1],))\n",
    "    # Add the input layer to the model\n",
    "    processed_atext = network(input_qtext)\n",
    "    processed_qtext = network(input_atext)\n",
    "\n",
    "    # Create a Lambda layer that calculates the euclidean distance of data points using the inputs\n",
    "    distance = Lambda(euclidean_distance, output_shape=(1,))([processed_atext, processed_qtext])\n",
    "    \n",
    "    # Create a classification layer with `sigmoid` function\n",
    "    prediction = Dense(1, activation='sigmoid', name= 'class_layer')(distance)\n",
    "\n",
    "    # Build a complete model w. the inputs and the prediction layers\n",
    "    model = Model([input_qtext, input_atext], prediction)\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=contrastive_loss, metrics = [\"accuracy\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune the hidden size with keras tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 1\n",
      "hidden_size (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [64, 128, 256], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "# Set up the tuner class to search\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel = siam_model,\n",
    "    objective = kt.Objective('val_accuracy', 'max'),\n",
    "    max_trials = 5,\n",
    "    num_initial_points = 2,\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "# Take a look at the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 03s]\n",
      "val_accuracy: 0.5801600813865662\n",
      "\n",
      "Best val_accuracy So Far: 0.5801600813865662\n",
      "Total elapsed time: 00h 00m 20s\n",
      "{'hidden_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masonphung/data_science/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 14 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1734</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1734</span>)      │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │    <span style=\"color: #00af00; text-decoration-color: #00af00\">115,200</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Sequential</span>)        │                   │            │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>], │\n",
       "│                     │                   │            │ sequential[<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ class_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1734\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1734\u001b[0m)      │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ sequential          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │    \u001b[38;5;34m115,200\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mSequential\u001b[0m)        │                   │            │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ sequential[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m], │\n",
       "│                     │                   │            │ sequential[\u001b[38;5;34m1\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ class_layer (\u001b[38;5;33mDense\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">115,202</span> (450.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m115,202\u001b[0m (450.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">115,202</span> (450.01 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m115,202\u001b[0m (450.01 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set early stopping settings\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3)\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(\n",
    "    [qtrain_tfidf, atrain_tfidf],\n",
    "    train_label,\n",
    "    validation_data = ([qval_tfidf, aval_tfidf], val_label),\n",
    "    epochs = 10,\n",
    "    shuffle = True,\n",
    "    callbacks = [early_stopping]\n",
    "    )\n",
    "\n",
    "# Print the best hyperparameters and model (Top 1)\n",
    "topN = 1\n",
    "for x in range(topN):\n",
    "    best_hp = tuner.get_best_hyperparameters(topN)[x]\n",
    "    print(best_hp.values)\n",
    "    print(tuner.get_best_models(topN)[x].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & test the optimal model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the model using train and val set\n",
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masonphung/data_science/lib/python3.11/site-packages/keras/src/models/functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['keras_tensor_9', 'keras_tensor_10']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5314 - loss: 0.1240 - val_accuracy: 0.5498 - val_loss: 0.1218\n",
      "Epoch 2/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.5539 - loss: 0.1095 - val_accuracy: 0.5712 - val_loss: 0.1208\n",
      "Epoch 3/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.7598 - loss: 0.0928 - val_accuracy: 0.5879 - val_loss: 0.1213\n",
      "Epoch 4/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8555 - loss: 0.0775 - val_accuracy: 0.5501 - val_loss: 0.1242\n",
      "Epoch 5/10\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8945 - loss: 0.0636 - val_accuracy: 0.5513 - val_loss: 0.1261\n",
      "\n",
      "Evaluate the model with the test set\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 445us/step - accuracy: 0.5681 - loss: 0.1210\n",
      "\n",
      "Test loss: 0.1236235573887825\n",
      "Test accuracy: 0.5535227656364441\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the best hp\n",
    "optimal_model = siam_model(best_hp)\n",
    "print('Fit the model using train and val set\\n')\n",
    "optimal_model.fit(\n",
    "    [qtrain_tfidf, atrain_tfidf], \n",
    "    train_label, \n",
    "    validation_data = ([qval_tfidf, aval_tfidf], val_label),\n",
    "    callbacks = [early_stopping], \n",
    "    epochs = 10, shuffle=True,  batch_size = 32\n",
    ")\n",
    "\n",
    "# Evaluate the model with test set\n",
    "print('\\nEvaluate the model with the test set')\n",
    "loss, acc =  optimal_model.evaluate([qtest_tfidf, atest_tfidf], test_label, batch_size = 32)\n",
    "\n",
    "print('\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "- Best accuracy is 0.55, the model is overfitting.\n",
    "- The two dense layers was represented as `sequential_1` (our base model), that why we don't see them in the summary above.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "\n",
    "- The predictions we get are probabilities.\n",
    "- The left value means the probability the machine \"thinks\" that the label is 0 and similarly, the right value shows us how \"sure\" it is with the 1 as the label.\n",
    "- We simply take a look at the right value, compare if it's larger than 0.5, which means that the model thinks that the label is more likely equal to 1, we mark it as True (else False).\n",
    "- As we have 0 and 1 as our label, simply convert T/F to 1/0 to match the original label format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 917us/step\n",
      "Failure Case Example\n",
      "--------------------\n",
      "qtext: What are the signs of an insulin overdose?\n",
      "atext: Your doctor may call it hypoglycemia.\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Classification prob: 0.527047336101532\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted labels\n",
    "preds = optimal_model.predict([qtest_tfidf, atest_tfidf])\n",
    "# Get the orinal values to see \"how sure\" the model is when classifying\n",
    "original_pred_label = preds.flatten()\n",
    "# We check if each predicted value > 0.5, then return T/F to 1/0\n",
    "# use flatten() to convert the array to 1D, similar to the test_label for easy comparison\n",
    "pred_label = (preds > 0.5).astype(int).flatten()\n",
    "\n",
    "# Compare each label\n",
    "incorrect_index = np.where(pred_label != test_label)[0]\n",
    "\n",
    "# Taking the 5th incorrect prediction as an example\n",
    "index = incorrect_index[4] \n",
    "\n",
    "# Take the values at the chosen index\n",
    "qtext = test_data.iloc[index]['qtext']\n",
    "atext = test_data.iloc[index]['atext']\n",
    "true_label = test_label[index]\n",
    "predicted_label = pred_label[index]\n",
    "original_predicted = original_pred_label[index]\n",
    "\n",
    "print(f\"Failure Case Example\")\n",
    "print(\"--------------------\")\n",
    "print(f\"qtext: {qtext}\")\n",
    "print(f\"atext: {atext}\")\n",
    "print(f\"True Label: {true_label}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "print(f\"Classification prob: {original_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "- tf.idf does not consider the semantic meaning of the sentence. This is quite important as we need to consider the general context of the sentence or the words in that sentence to be able to understand the correct meaning.\n",
    "\n",
    "- In this case:\n",
    "    - The model may catched the word `insulin` in the question.\n",
    "    - The answer includes `doctor`, `hypoglycemia`, which are also related to `insulin`.\n",
    "    - Therefore, the model may think that they are similar and labeled `1`. Classification probability is 0.73, which means that the model is pretty confident about its guess.\n",
    "\n",
    "- Potential solution:\n",
    "    - We can use word embedding as this feature helps the model to better understand the semantic relationships between words of a sentence.\n",
    "\n",
    "- Moreover, the model is overfitting, so we can reduce the hidden size to reduce the complexity or add dropout layers to regularize our model.\n",
    "\n",
    "- P/S: The actual case would change due to the randomness of the tensorflow system, but the idea should be similar.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2 (12 marks): Transformer\n",
    "\n",
    "In this task, let's use Transformer to predict whether two sentences are related or not. Implement a simple Transformer neural network that meets the following requirements:\n",
    "\n",
    "1. ✅(1 mark) Each input for this model should be a concatenation of qtext and atext. Use [SEP] to separate qtext and atext, e.g., \"Can high blood pressure bring on heart failure? [SEP] Hypertensive heart disease is the No.\" You need to pad the input to a fixed length. How do you determine a suitable length?\n",
    "2. ✅(1.5 marks) Choose a suitable tokenizer and justify your choice.\n",
    "3. ✅(1 mark) An embedding layer that generates embedding vectors of the sentence text into size 128. Remember to add position embedding.\n",
    "4. ✅(1 mark) One transformer encoder layer, you need to find a hidden dimension in {64, 128, 256}. Use 3 heads in MultiHeadAttention.\n",
    "5. ✅(1 mark) Do we need a transformer decoder layer for this task? If yes, find a hidden dimension in {64, 128, 256} and use 3 heads in MultiHeadAttention. If no, explain why.\n",
    "6. ✅(0.5 marks) 1 hidden layer with size 256 and ReLU activation function.\n",
    "7. ✅(0.5 marks) 1 output layer with size 2 for binary classification to predict whether two inputs are related or not. \n",
    "8. ✅(1 mark) Choose a suitable loss to train the model\n",
    "9. ✅(1 mark) Report your best accuracy on the test split.\n",
    "10. ✅(1.5 marks) Give an example of a failure case, and explain the possible reason and discuss a potential solution.\n",
    "11. (1 mark) Good coding style as explained in the above Assessment Section.\n",
    "12. (1 mark) Correctly feeding data into your model, and correctly training and testing of your models.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer some of the required questions\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "\n",
    "- 1. The max length is the maximum length that the model takes for each sentence, I think the suitable length would be a value that we can cover the length of most the sentences. I'll use median to determine this value later\n",
    "\n",
    "- 2. We choose BERT tokenizer as it allow us to detect two input sentences as one independently with the help of [SEP]. The tokenizer of the ‘bert-base-uncased’ model is also suitable for text classification.\n",
    "\n",
    "\n",
    "- 4. We don't need a Decoder layer.\n",
    "        - As we are doing a classification task, each of our output is simply a single probability value.\n",
    "        - A decoder layer is usually used when we need our output to be in sequence like in generation tasks or multiple regressions.\n",
    "        - Use a decoder layer is excess in our case.\n",
    "\n",
    "- 8. Cross-entropy optimizes directly for class probabilities, focusing on correctly classifying each input pair as related or unrelated. I use `sparse_categorical_crossentropy` as we have 2 output neurons here (instead of binary with a single neuron).\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To create the Transformer model\n",
    "- I use the classes that we learned from the Practical, the `TokenAndPositionEmbedding` class for the embedding layer and the `TransformerBlock` to use as a transformer encoder layer.\n",
    "\n",
    "- Use BERT tokenizer as explained above.\n",
    "\n",
    "- Set required parameters (`embed_dim`, `num_heads`) based on the requirements. If I have to determine myself (`max_length`, `vocab_size`), I either explain when I do it or in the above answers.\n",
    "\n",
    "- Data preprocessing\n",
    "    - Concatenate data of 3 sets\n",
    "    - Call function `data_preprocess` to convert them to list, tokenize then pad the sentences.\n",
    "    - Convert the labels to array format\n",
    "\n",
    "- Build the model following the requirements, and:\n",
    "    - Tune hidden dimension using `keras_tuner`. Use `BayesianOptimization` as it is suitable for our case.\n",
    "    - I had to use `GlobalAveragePooling1D()(x)` as we expect a single output label. Without this the `TransformerBlock` would have a sequence of output, which does not fit our output layer.\n",
    "    - Use the optimal hidden size to fit the train/val data and evaluate with the test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding layer\n",
    "- Source at keras documentation page and the practical class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenAndPositionEmbedding(Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(start=0, stop=maxlen, step=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformer block\n",
    "- Source at keras documentation page and the practical class\n",
    "- We'll later use this transformer block as our transformer encoder layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.ffn = Sequential(\n",
    "            [\n",
    "                Dense(ff_dim, activation=\"relu\"),\n",
    "                Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = Dropout(rate)\n",
    "        self.dropout2 = Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BERT tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use BERT pretrained tokenizer\n",
    "tokenizer = keras_nlp.models.BertTokenizer.from_preset(\"bert_base_en_uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate data\n",
    "\n",
    "- Based on the requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate `qtext` and `atext` together, seperate them with [SEP]\n",
    "text_train = train_data['qtext'] + \" [SEP] \" + train_data['atext']\n",
    "text_val = val_data['qtext'] + \" [SEP] \" + val_data['atext']\n",
    "text_test = test_data['qtext'] + \" [SEP] \" + test_data['atext']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set parameters\n",
    "\n",
    "These parameters are set mostly based on the requirements of the question.\n",
    "- `max_length` is 50 as this is close to our frequent sentence length (median) and I think that it can cover most of our sentences.\n",
    "- `vocab_size` is equal to the vocabulary size of the BERT tokenizer but we add 1 for padding index 0.\n",
    "\n",
    "We can do the below steps to check for the median of all the sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median of tokenized sentence lengths: 35.0\n"
     ]
    }
   ],
   "source": [
    "# Check the median by using the tokenized data length\n",
    "# Convert data to list\n",
    "text = text_train.tolist()\n",
    "# Tokenize each sentence in the data\n",
    "text = tokenizer.tokenize(text)\n",
    "\n",
    "# Print the length of each tokenized sentence\n",
    "token_lengths = [len(sentence) for sentence in text]\n",
    "med_length = np.median(token_lengths)\n",
    "print(\"Median of tokenized sentence lengths:\", med_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "embed_dim = 128\n",
    "num_heads = 3\n",
    "vocab_size = tokenizer.vocabulary_size()+1 # +1 for padding index[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data pre-processing - Set parameters\n",
    "\n",
    "- Concatenate data of 3 sets\n",
    "- Call function `data_preprocess` to convert them to list, tokenize then pad the sentences.\n",
    "- Convert the labels to array format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocess(text, tokenizer, maxlen):\n",
    "    \"\"\"\n",
    "    Preprocess the data by coverting each variable into list, tokenize with the predefined tokenizer.\n",
    "    Then pad each sentence in the data to the max length allowed.\n",
    "    \n",
    "    Parameters\n",
    "    - text (pd.Series): text data to be preprocessed\n",
    "    - tokenizer (Tokenizer): A tokenizer instance used to convert text into sequences of token IDs.\n",
    "    - maxlen (int): The maximum length for padding each sequence. Sequences shorter than this length are padded,\n",
    "                    and longer ones are truncated.\n",
    "\n",
    "    Returns:\n",
    "    - numpy.ndarray: A 2D array where each row is a padded token sequence of a fixed length `maxlen`.\n",
    "    \"\"\"\n",
    "    # Convert data to list\n",
    "    text = text.tolist()\n",
    "    # Tokenize each sentence in the data\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    # Pad the sentences\n",
    "    text = sequence.pad_sequences(text, maxlen = maxlen)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data with our defined function\n",
    "text_train = data_preprocess(text_train, tokenizer, max_length)\n",
    "text_val = data_preprocess(text_val, tokenizer, max_length)\n",
    "text_test = data_preprocess(text_test, tokenizer, max_length)\n",
    "\n",
    "# Process labels\n",
    "label_train = np.array(train_data['label'].tolist())\n",
    "label_val = np.array(val_data['label'].tolist())\n",
    "label_test = np.array(test_data['label'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create model\n",
    "- Simply create each layer then add them to the model one by one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transformer_model(hp):\n",
    "    \"\"\"\n",
    "    Builds and compiles a Transformer classification model.\n",
    "    Hidden size is ready to be hp tuned.\n",
    "\n",
    "    Parameters:\n",
    "    - hp(HyperParameters object): For tuning model configurations.\n",
    "\n",
    "    Returns:\n",
    "        A compiled Transformer classfication model, with hidden size ready to be tuned with Keras tuner.\n",
    "    \"\"\"\n",
    "    # Create an input layer\n",
    "    input = Input(shape = (max_length,))\n",
    "\n",
    "    # Create an embedding layer and use it with the input\n",
    "    embedding_layer = TokenAndPositionEmbedding(max_length, vocab_size, embed_dim)\n",
    "    model = embedding_layer(input)\n",
    "    \n",
    "    # Tune hidden size for the transformer block (encoder layer)\n",
    "    hidden_size = hp.Choice(name = \"hidden_size\", values = [64,128,256])\n",
    "    # Create a transformer block layer and add to the model\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, hidden_size)\n",
    "    model = transformer_block(model)\n",
    "    \n",
    "    # A GAP1D layer to flatten the output of transformer block to fit our output layer\n",
    "    model = GlobalAveragePooling1D()(model)\n",
    "    \n",
    "    # Add a 256 units hidden layer and the output layer\n",
    "    model = Dense(units = 256, activation=\"relu\")(model)\n",
    "    outputs = Dense(units = 2, activation=\"softmax\")(model)\n",
    "\n",
    "    trans_model = Model(inputs=input, outputs=outputs)\n",
    "    trans_model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "   \n",
    "    return trans_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space summary\n",
      "Default search space size: 1\n",
      "hidden_size (Choice)\n",
      "{'default': 64, 'conditions': [], 'values': [64, 128, 256], 'ordered': True}\n"
     ]
    }
   ],
   "source": [
    "# Set up the tuner class to search\n",
    "tuner = kt.BayesianOptimization(\n",
    "    hypermodel = transformer_model,\n",
    "    objective = kt.Objective('val_accuracy', 'max'),\n",
    "    max_trials = 5,\n",
    "    num_initial_points = 2,\n",
    "    overwrite = True\n",
    ")\n",
    "\n",
    "# Take a look at the search space\n",
    "tuner.search_space_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 01m 39s]\n",
      "val_accuracy: 0.5993208885192871\n",
      "\n",
      "Best val_accuracy So Far: 0.6495270729064941\n",
      "Total elapsed time: 00h 06m 09s\n",
      "{'hidden_size': 64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/masonphung/data_science/lib/python3.11/site-packages/keras/src/saving/saving_lib.py:719: UserWarning: Skipping variable loading for optimizer 'adam', because it has 2 variables whereas the saved optimizer has 46 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">3,913,344</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TokenAndPositionEmbedding</span>)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">214,976</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TransformerBlock</span>)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">33,024</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">514</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ token_and_position_embedding    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │     \u001b[38;5;34m3,913,344\u001b[0m │\n",
       "│ (\u001b[38;5;33mTokenAndPositionEmbedding\u001b[0m)     │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ transformer_block               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        │       \u001b[38;5;34m214,976\u001b[0m │\n",
       "│ (\u001b[38;5;33mTransformerBlock\u001b[0m)              │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m33,024\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m514\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,161,858</span> (15.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,161,858\u001b[0m (15.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,161,858</span> (15.88 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m4,161,858\u001b[0m (15.88 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Set early stopping settings\n",
    "early_stopping = EarlyStopping(monitor = 'val_loss', min_delta = 0, patience = 3)\n",
    "# Search for the best hyperparameters\n",
    "tuner.search(\n",
    "    text_train,\n",
    "    label_train,\n",
    "    validation_data = (text_val, label_val),\n",
    "    epochs = 10,\n",
    "    shuffle = True,\n",
    "    callbacks = [early_stopping]\n",
    "    )\n",
    "\n",
    "# Print the best hyperparameters and model (Top 1)\n",
    "topN = 1\n",
    "for x in range(topN):\n",
    "    best_hp = tuner.get_best_hyperparameters(topN)[x]\n",
    "    print(best_hp.values)\n",
    "    print(tuner.get_best_models(topN)[x].summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit the model using train and val set\n",
      "\n",
      "Epoch 1/5\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 61ms/step - accuracy: 0.5931 - loss: 0.6589 - val_accuracy: 0.5239 - val_loss: 0.7447\n",
      "Epoch 2/5\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 56ms/step - accuracy: 0.8128 - loss: 0.4256 - val_accuracy: 0.6168 - val_loss: 0.7646\n",
      "Epoch 3/5\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 58ms/step - accuracy: 0.8878 - loss: 0.2890 - val_accuracy: 0.5886 - val_loss: 0.8078\n",
      "Epoch 4/5\n",
      "\u001b[1m294/294\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 55ms/step - accuracy: 0.9234 - loss: 0.2157 - val_accuracy: 0.5501 - val_loss: 1.3668\n",
      "\n",
      "Evaluate the model with the test set\n",
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 14ms/step - accuracy: 0.5815 - loss: 1.3924\n",
      "\n",
      "Test loss: 1.3522511720657349\n",
      "Test accuracy: 0.5772674083709717\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the best hp\n",
    "optimal_trans_model = transformer_model(best_hp)\n",
    "print('Fit the model using train and val set\\n')\n",
    "optimal_trans_model.fit(\n",
    "    text_train, \n",
    "    label_train, \n",
    "    validation_data = (text_val, label_val),\n",
    "    callbacks = [early_stopping], \n",
    "    epochs = 5, shuffle=True,  batch_size = 32\n",
    ")\n",
    "\n",
    "# Evaluate the model with test set\n",
    "print('\\nEvaluate the model with the test set')\n",
    "loss, acc =  optimal_trans_model.evaluate(text_test, label_test, batch_size = 32)\n",
    "print('\\nTest loss:', loss)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "best accuracy is about 0.58, the model is overfitting.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Analysis\n",
    "- We have to deal with the predictions differently because we are having a 2D output for each prediction.\n",
    "- The left value means the probability the machine thinks that the label is 0 and similarly, the right value shows us how sure it is with the 1 as the label.\n",
    "- We simply take a look at the right value, compare if it's larger than 0.5, which means that the model thinks that the label is more likely equal to 1, we mark it as True (else False).\n",
    "- As we have 0 and 1 as our label, simply convert T/F to 1/0 to match the original label format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m161/161\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step\n",
      "Failure Case Example\n",
      "--------------------\n",
      "qtext: What are the signs of an insulin overdose?\n",
      "atext: Your doctor may call it hypoglycemia.\n",
      "True Label: 0\n",
      "Predicted Label: 1\n",
      "Classification prob: 0.9908889532089233\n"
     ]
    }
   ],
   "source": [
    "# Get the predicted labels\n",
    "preds = optimal_trans_model.predict(text_test)\n",
    "\n",
    "# Use the probability for the \"related\" class (second column) as the prediction confidence\n",
    "original_pred_label = preds[:, 1]\n",
    "\n",
    "# We check if the right prob value > 0.5, then return T/F (but then converts to 1/0)\n",
    "# Convert probs to binary labels: 1 if the prob > 0.5, else 0\n",
    "pred_label = (preds[:, 1] > 0.5).astype(int)\n",
    "\n",
    "# Compare each label with the true test label\n",
    "incorrect_index = np.where(pred_label != label_test)[0]\n",
    "\n",
    "# Taking the 5th incorrect prediction as an example\n",
    "index = incorrect_index[4] \n",
    "\n",
    "# Take the values at the chosen index\n",
    "qtext = test_data.iloc[index]['qtext']\n",
    "atext = test_data.iloc[index]['atext']\n",
    "true_label = label_test[index]\n",
    "predicted_label = pred_label[index]\n",
    "original_predicted = original_pred_label[index]\n",
    "\n",
    "print(f\"Failure Case Example\")\n",
    "print(\"--------------------\")\n",
    "print(f\"qtext: {qtext}\")\n",
    "print(f\"atext: {atext}\")\n",
    "print(f\"True Label: {true_label}\")\n",
    "print(f\"Predicted Label: {predicted_label}\")\n",
    "print(f\"Classification prob: {original_predicted}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "\n",
    "- Aside from the semantic reasons that we could assume from words in a similar topic appearance (insulin/hypoglycemia), we can see that the model is highly overfitting.\n",
    "\n",
    "- As I observe our settings (which given as requirements), the hidden sizes of dense layers are relatively high (up to 256 at the final dense layer), which can be our main reason. With high hidden size, the model is more complex and is better at classifying the set (we can clearly see how it is faster to get to a high acc in train set). \n",
    "\n",
    "- However, this would make it remember some specific details about the train set, \"thinks\" that it is the correct patterns. As a results, the model would have worse results with other sets, including the val/test set and is overfitting.\n",
    "\n",
    "- Potential solution:\n",
    "    - Reduce the hidden size, which is better for small size data like the one we have. Avoid too complex model when it's not necessary.\n",
    "    - We can add dropout layers, which randomly deactive model neurons on training, better regularize our model.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "- https://datascience.stackexchange.com/questions/85847/role-of-decoder-in-transformer#:~:text=About%20needing%20the%20decoder%20in,task%2C%20the%20encoder%20would%20suffice.\n",
    "\n",
    "- https://levelup.gitconnected.com/understanding-transformers-from-start-to-end-a-step-by-step-math-example-16d4e64e6eb1\n",
    "\n",
    "- https://keras.io/examples/nlp/text_classification_from_scratch/\n",
    "\n",
    "- http://jalammar.github.io/illustrated-bert/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "A2_solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "data_science",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
